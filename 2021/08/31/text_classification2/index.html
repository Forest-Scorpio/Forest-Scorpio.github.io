<!DOCTYPE html><html lang="zh_CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>文本分类的深度学习方法【二】 | 森森然的部落格</title><meta name="keywords" content="文本分类,深度学习,深层模型,BERT"><meta name="author" content="SeekingBlue"><meta name="copyright" content="SeekingBlue"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="DPCNN 😊 论文：https:&#x2F;&#x2F;ai.tencent.com&#x2F;ailab&#x2F;media&#x2F;publications&#x2F;ACL3-Brady.pdf代码：https:&#x2F;&#x2F;github.com&#x2F;649453932&#x2F;Chinese-Text-Classification-Pytorch  ACL 2017 年，腾讯 AI-lab 提出了 Deep Pyramid Convolutional Neura">
<meta property="og:type" content="article">
<meta property="og:title" content="文本分类的深度学习方法【二】">
<meta property="og:url" content="http://example.com/2021/08/31/text_classification2/index.html">
<meta property="og:site_name" content="森森然的部落格">
<meta property="og:description" content="DPCNN 😊 论文：https:&#x2F;&#x2F;ai.tencent.com&#x2F;ailab&#x2F;media&#x2F;publications&#x2F;ACL3-Brady.pdf代码：https:&#x2F;&#x2F;github.com&#x2F;649453932&#x2F;Chinese-Text-Classification-Pytorch  ACL 2017 年，腾讯 AI-lab 提出了 Deep Pyramid Convolutional Neura">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.vennify.ai/content/images/size/w1140/2021/06/text-classification--1--1.png">
<meta property="article:published_time" content="2021-08-31T03:00:00.000Z">
<meta property="article:modified_time" content="2021-09-03T06:52:45.689Z">
<meta property="article:author" content="SeekingBlue">
<meta property="article:tag" content="文本分类">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="深层模型">
<meta property="article:tag" content="BERT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.vennify.ai/content/images/size/w1140/2021/06/text-classification--1--1.png"><link rel="shortcut icon" href="/img/capricorn.png"><link rel="canonical" href="http://example.com/2021/08/31/text_classification2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文本分类的深度学习方法【二】',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2021-09-03 14:52:45'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/bilibiliBanner.css"  media="defer" onload="this.media='screen'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-butterfly-clock/lib/clock.min.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="wizard-scene"><div class="wizard-objects"><div class="wizard-square"></div><div class="wizard-circle"></div><div class="wizard-triangle"></div></div><div class="wizard"><div class="wizard-body"></div><div class="wizard-right-arm"><div class="wizard-right-hand"></div></div><div class="wizard-left-arm"><div class="wizard-left-hand"></div></div><div class="wizard-head"><div class="wizard-beard"></div><div class="wizard-face"><div class="wizard-adds"></div></div><div class="wizard-hat"><div class="wizard-hat-of-the-hat"></div><div class="wizard-four-point-star --first"></div><div class="wizard-four-point-star --second"></div><div class="wizard-four-point-star --third"></div></div></div></div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/tx.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">4</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">12</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://www.vennify.ai/content/images/size/w1140/2021/06/text-classification--1--1.png)"><nav id="nav"><span id="blog_name"><a id="site-name" onclick="btf.scrollToDest(0, 500)" data-title="欢迎光临森森然小屋，愿你有愉快的一天">文本分类的深度学习方法【二】</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">文本分类的深度学习方法【二】</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-08-31T03:00:00.000Z" title="Created 2021-08-31 11:00:00">2021-08-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-09-03T06:52:45.689Z" title="Updated 2021-09-03 14:52:45">2021-09-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">自然语言处理</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">6.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>24min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文本分类的深度学习方法【二】"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="DPCNN-😊"><a href="#DPCNN-😊" class="headerlink" title="DPCNN 😊"></a>DPCNN 😊</h1><blockquote>
<p>论文：<a target="_blank" rel="noopener" href="https://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf">https://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf</a><br>代码：<a target="_blank" rel="noopener" href="https://github.com/649453932/Chinese-Text-Classification-Pytorch">https://github.com/649453932/Chinese-Text-Classification-Pytorch</a></p>
</blockquote>
<p><code>ACL</code> 2017 年，腾讯 <code>AI-lab</code> 提出了 <code>Deep Pyramid Convolutional Neural Networks for Text Categorization(DPCNN)</code>，由于 <code>TextCNN</code> 不能通过卷积获得文本的长距离依赖关系，而论文中 <code>DPCNN</code> 通过不断加深网络，可以抽取长距离的文本依赖关系。实验证明在不增加太多计算成本的情况下，增加网络深度就可以获得最佳的准确率。‍</p>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5cdd94fd35d64417a0cd2cd0b0b6116f~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<h2 id="Region-embedding"><a href="#Region-embedding" class="headerlink" title="Region embedding"></a>Region embedding</h2><p><code>TextCNN</code> 的包含多尺寸卷积滤波器的卷积层的卷积结果称之为 <code>Region embedding</code>，意思就是对一个文本区域/片段（比如<code>3-gram</code>）进行一组卷积操作后生成的 <code>embedding</code>。</p>
<h2 id="卷积和全连接的权衡"><a href="#卷积和全连接的权衡" class="headerlink" title="卷积和全连接的权衡"></a>卷积和全连接的权衡</h2><p>产生 <code>region embedding</code> 后，按照经典的 <code>TextCNN</code> 的做法的话，就是从每个特征图中挑选出最有代表性的特征，也就是直接应用全局最大池化层，这样就生成了这段文本的特征向量，假如卷积滤波器的 <code>size</code> 有 <code>(3，4，5)</code> 这三种，每种 <code>size</code> 包含 $100$ 个卷积核，那么当然就会产生 $3*100$ 幅特征图，然后将<code>max-over-time-pooling</code> 操作应用到每个特征图上，于是文本的特征向量即 $3 \times 100=300$ 维。</p>
<p><code>TextCNN</code> 这样做的意义本质上与词袋模型的经典文本分类模型没本质区别，只不过 <code>one-hot</code> 到 <code>word embedding</code> 表示的转变避免了词袋模型遭遇的数据稀疏问题。<code>TextCNN</code> 本质上收益于词向量的引入带来的近义词有相近向量表示的 <code>bonus</code>，同时 <code>TextCNN</code> 可以较好的利用词向量中近义关系。</p>
<blockquote>
<p>但是文本中的远距离信息在 <code>TextCNN</code> 中依然难以学习。</p>
</blockquote>
<h2 id="等长卷积"><a href="#等长卷积" class="headerlink" title="等长卷积"></a>等长卷积</h2><p>假设输入的序列长度为 $n$，卷积核大小为 $m$，步长为 $s$，输入序列两端各填补 $p$ 个零，那么该卷积层的输出序列为 $\frac{(n-m+2p)}{s}+1$。</p>
<ul>
<li><p>窄卷积🍊:步长 $s=1$ ，两端不补零，即 $p=0$，卷积后输出长度为 $n-m+1$。</p>
</li>
<li><p>宽卷积🍊:步长 $s=1$，两端补零 $p=m-1$，卷积后输出长度 $n+m-1$。</p>
</li>
<li><p>等长卷积🍊: 步长 $s=1$，两端补零 $p=(m-1)/2$，卷积后输出长度为 $n$。</p>
</li>
</ul>
<p>我们将输入输出序列的第 $n$ 个 <code>embedding</code> 称为第 $n$ 个词位，那么这时 <code>size=n</code> 的卷积核产生的等长卷积的意义就是将输入序列的每个词位及其左右 $\frac{n-1}{2}$ 个词的上下文信息压缩为该词位的 <code>embedding</code>，产生了每个词位的被上下文信息修饰过的更高 <code>level</code>、更加准确的语义。想要克服 <code>TextCNN</code> 的缺点，捕获长距离模式，显然就要用到深层 <code>CNN</code>。</p>
<p>直接等长卷积堆等长卷积会让每个词位包含进去越来越多，越来越长的上下文信息，这种方式会让网络层数变得非常非常非常深，但是这种方式太笨重。不过，既然等长卷积堆等长卷积会让每个词位的<code>embedding</code> 描述语义描述的更加丰富准确，可以适当的堆两层来提高词位 <code>embedding</code>的表示的丰富性。</p>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/304e0f83debe45fab2883e8a29f92712~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<h2 id="固定-feature-map-的数量"><a href="#固定-feature-map-的数量" class="headerlink" title="固定 feature map 的数量"></a>固定 feature map 的数量</h2><p>在表示好每个词位的语义后，很多邻接词或者邻接 <code>ngram</code> 的词义是可以合并，例如 “小明 人 不要 太好” 中的 “不要” 和 “太好” 虽然语义本来离得很远，但是作为邻接词“不要太好”出现时其语义基本等价为“很好”，完全可以把“不要”和“太好”的语义进行合并。同时，合并的过程完全可以在原始的 <code>embedding space</code> 中进行的，原文中直接把“不要太好”合并为“很好”是很可以的，完全没有必要动整个语义空间。</p>
<p>实际上，相比图像中这种从“点、线、弧”这种 <code>low-level</code> 特征到“眼睛、鼻子、嘴”这种 <code>high-level</code> 特征的明显层次性的特征区分，文本中的特征进阶明显要扁平的多，即从单词（<code>1gram</code>）到短语再到 <code>3gram</code>、<code>4gram</code> 的升级，其实很大程度上均满足“语义取代”的特性。而图像中就很难发生这种“语义取代”现象。因此，<code>DPCNN</code> 与 <code>ResNet</code> 很大一个不同就是，<strong>在 <code>DPCNN</code> 中固定死了 <code>feature map</code> 的数量</strong>，也就是固定住了 <code>embedding space</code> 的维度（为了方便理解，以下简称语义空间），使得网络有可能让整个邻接词（邻接 <code>ngram</code>）的合并操作在原始空间或者与原始空间相似的空间中进行（当然，网络在实际中会不会这样做是不一定的，只是提供了这么一种条件）。也就是说，整个网络虽然形状上来看是深层的，但是从语义空间上来看完全可以是扁平的。</p>
<h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2><p>每经过一个 $size=3,stride=2$ 的池化层(简称 $1/2$ 池化层)，序列的长度就被压缩成了原来的一半。这样同样是 $size=3$ 的卷积核，每经过一个 $1/2$ 池化层后，其能感知到的文本片段就比之前长了一倍。例如之前是只能感知 $3$ 个词位长度的信息，经过 $1/2$ 池化层后就能感知 $6$ 个词位长度的信息，这时把 $1/2$ 池化层和 <code>size=3</code> 的卷积层组合起来如图：</p>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1b697f3e79d34f8382862416a0495de2~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<h2 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h2><p>在初始化深度 <code>CNN</code> 时，往往各层权重都是初始化为一个很小的值，这就导致最开始的网络中，后续几乎每层的输入都是接近 $0$，这时网络的输出自然是没意义的，而这些小权重同时也阻碍了梯度的传播，使得网络的初始训练阶段往往要迭代很久才能启动。同时，就算网络启动完成，由于深度网络中仿射矩阵近似连乘，训练过程中网络也非常容易发生梯度爆炸或弥散问题（虽然由于非共享权重，深度<code>CNN</code> 网络比 <code>RNN</code> 网络要好点）。针对深度 <code>CNN</code> 网络的梯度弥散问题 <code>ResNet</code> 中提出的<code>shortcut-connection\skip-connection\residual-connection</code>（残差连接）就是一种非常简单、合理、有效的解决方案。</p>
<p><img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d6b32d4001dd4a9db3cf71cb2256ae2a~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<h2 id="模型结构和代码"><a href="#模型结构和代码" class="headerlink" title="模型结构和代码"></a>模型结构和代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DPCNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        config.vocab_size：词表大小</span></span><br><span class="line"><span class="string">        config.embedding_size：词向量维度</span></span><br><span class="line"><span class="string">        config.num_labels: 类别数</span></span><br><span class="line"><span class="string">        config.n_filters: 卷积核个数（对应2维卷积的通道数）</span></span><br><span class="line"><span class="string">        config.filter_sizes: 卷积核的尺寸(3,4,5)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(DPCNN, self).__init__()</span><br><span class="line">        self.embedding_size = config.embedding_size</span><br><span class="line">        self.embedding = nn.Embedding(config.vocab_size, config.embedding_size)</span><br><span class="line"></span><br><span class="line">        self.conv_region = nn.Conv2d(<span class="number">1</span>, config.n_filters, (<span class="number">3</span>, self.embedding_size), stride=<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Conv2d(config.n_filters, config.n_filters, (<span class="number">3</span>, <span class="number">1</span>), stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.max_pool = nn.MaxPool2d(kernel_size=(<span class="number">3</span>, <span class="number">1</span>), stride=<span class="number">2</span>)</span><br><span class="line">        self.padding1 = nn.ZeroPad2d((<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># top bottom</span></span><br><span class="line">        self.padding2 = nn.ZeroPad2d((<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>))  <span class="comment"># bottom</span></span><br><span class="line"></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.fc = nn.Linear(config.n_filters, config.num_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids</span>):</span></span><br><span class="line">        <span class="comment"># [batch size, seq len, emb dim]</span></span><br><span class="line">        x = self.embedding(input_ids) </span><br><span class="line">        <span class="comment"># [batch size, 1, seq len, emb dim]</span></span><br><span class="line">        x = x.unsqueeze(<span class="number">1</span>).to(torch.float32) </span><br><span class="line">        <span class="comment"># [batch size, n_filters, seq len-3+1, 1]</span></span><br><span class="line">        x = self.conv_region(x)  </span><br><span class="line">        <span class="comment"># [batch size, n_filters, seq len, 1]</span></span><br><span class="line">        x = self.padding1(x)  </span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="comment"># [batch size, n_filters, seq len-3+1, 1]</span></span><br><span class="line">        x = self.conv(x) </span><br><span class="line">        <span class="comment"># [batch size, n_filters, seq len, 1]</span></span><br><span class="line">        x = self.padding1(x)  </span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        <span class="comment"># [batch size, n_filters, seq len-3+1, 1]</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="comment"># [batch size, n_filters, 1, 1]</span></span><br><span class="line">        <span class="keyword">while</span> x.size()[<span class="number">2</span>] &gt;= <span class="number">2</span>:</span><br><span class="line">            x = self._block(x)  </span><br><span class="line">        <span class="comment"># [batch size, n_filters]</span></span><br><span class="line">        x_embedding = x.squeeze()  </span><br><span class="line">        <span class="comment"># [batch_size, 1]</span></span><br><span class="line">        x = self.fc(x_embedding)  </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_block</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.padding2(x)</span><br><span class="line">        px = self.max_pool(x)</span><br><span class="line"></span><br><span class="line">        x = self.padding1(px)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line">        x = self.padding1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Short Cut</span></span><br><span class="line">        x = x + px</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h1 id="TextRCNN-😊"><a href="#TextRCNN-😊" class="headerlink" title="TextRCNN 😊"></a>TextRCNN 😊</h1><blockquote>
<p>论文：<a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.5555/2886521.2886636">https://dl.acm.org/doi/10.5555/2886521.2886636</a><br>代码：<a target="_blank" rel="noopener" href="https://github.com/649453932/Chinese-Text-Classification-Pytorch">https://github.com/649453932/Chinese-Text-Classification-Pytorch</a></p>
</blockquote>
<p><code>RNN</code> 和 <code>CNN</code> 作为文本分类问题的主要模型架构，都存在各自的优点及局限性。</p>
<ul>
<li><p><code>RNN</code> 擅长处理序列结构，能够考虑到句子的上下文信息，但 <code>RNN</code> 属于 <code>biased model</code>，一个句子中越往后的词重要性越高，这有可能影响最后的分类结果，因为对句子分类影响最大的词可能处在句子任何位置。</p>
</li>
<li><p><code>CNN</code> 属于无偏模型，能够通过最大池化获得最重要的特征，但是 <code>CNN</code> 的滑动窗口大小不容易确定，选的过小容易造成重要信息丢失，选的过大会造成巨大参数空间。</p>
</li>
</ul>
<p>为了解决二者的局限性，这篇文章提出了一种新的网络架构，用双向循环结构获取上下文信息，这比传统的基于窗口的神经网络更能减少噪声，而且在学习文本表达时可以大范围的保留词序。其次使用最大池化层获取文本的重要部分，自动判断哪个特征在文本分类过程中起更重要的作用。</p>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/396b3ac302114add92c8edecf7770060~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<h2 id="单词表示学习"><a href="#单词表示学习" class="headerlink" title="单词表示学习"></a>单词表示学习</h2><p>作者提出将单词的左上下文、右上下文、单词本身结合起来作为单词表示。作者使用了双向 <code>RNN</code> 来分别提取句子的上下文信息。公式如下:</p>
<script type="math/tex; mode=display">
\begin{array}{l}
c_{l}\left(w_{i}\right)=f\left(W^{(l)} c_{l}\left(w_{i-1}\right)+W^{(s l)} e\left(w_{i-1}\right)\right)  \\
c_{r}\left(w_{i}\right)=f\left(W^{(r)} c_{r}\left(w_{i+1}\right)+W^{(s r)} e\left(w_{i+1}\right)\right)
\end{array}</script><p>其中，$c_l(w_i)$ 代表单词 $w_i$ 的左上下文，$c_l(w_i)$ 由上一个单词的左上下文 $c_l(w_{i-1})$ 和 上一个单词的词嵌入向量 $e(w_{i-1})$ 计算得到，所有句子第一个单词的左侧上下文使用相同的共享参数 $c_l(w_1)$。</p>
<p>$W^{(l)},W^{(sl)}$ 用于将上一个单词的左上下文语义和上一个单词的语义结合到单词 $w_i$ 的左上下文表示中。右上下文的处理与左上下文完全相同，同样所有句子最后一个单词的右侧上下文使用相同的共享参数 $c_r(w_n)$。 得到句子中每个单词的左上下文表示和右上下文表示后，就可以定义单词  $w_i$ 的表示如下</p>
<script type="math/tex; mode=display">
\boldsymbol{x}_{i}=\left[\boldsymbol{c}_{l}\left(w_{i}\right) ; \boldsymbol{e}\left(w_{i}\right) ; \boldsymbol{c}_{r}\left(w_{i}\right)\right]</script><p>实际就是单词$w_i$，单词的词嵌入表示向量 $e(w_i)$ 以及单词的右上下文向量 $c_e(w_i)$ 的拼接后的结果。得到 $w_i$ 的表示$x_i$后，就可以输入激活函数得到$w_i$的潜在语义向量 $y_i^{(2)}$ 。</p>
<script type="math/tex; mode=display">\boldsymbol{y}_{i}^{(2)}=\tanh \left(W^{(2)} \boldsymbol{x}_{i}+\boldsymbol{b}^{(2)}\right)</script><h2 id="文本表示学习"><a href="#文本表示学习" class="headerlink" title="文本表示学习"></a>文本表示学习</h2><p>经过卷积层后，获得了所有词的表示，首先对其进行最大池化操作，最大池化可以帮助找到句子中最重要的潜在语义信息。</p>
<script type="math/tex; mode=display">
\boldsymbol{y}^{(3)}=\max _{i=1}^{n} \boldsymbol{y}_{i}^{(2)}</script><p>然后经过全连接层得到文本的表示，最后通过 <code>softmax</code> 层进行分类。</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\boldsymbol{y}^{(4)}=W^{(4)} \boldsymbol{y}^{(3)}+\boldsymbol{b}^{(4)}\\
&p_{i}=\frac{\exp \left(\boldsymbol{y}_{i}^{(4)}\right)}{\sum_{k=1}^{n} \exp \left(\boldsymbol{y}_{k}^{(4)}\right)}
\end{aligned}</script><h2 id="模型结构和代码-1"><a href="#模型结构和代码-1" class="headerlink" title="模型结构和代码"></a>模型结构和代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RCNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        config.vocab_size：词表大小</span></span><br><span class="line"><span class="string">        config.embedding_size：词向量维度</span></span><br><span class="line"><span class="string">        config.num_labels: 类别数</span></span><br><span class="line"><span class="string">        config.hidden_dim: rnn隐藏层的维度</span></span><br><span class="line"><span class="string">        config.n_layers: rnn层数</span></span><br><span class="line"><span class="string">        config.rnn_type: rnn类型，包括[&#x27;lstm&#x27;, &#x27;gru&#x27;, &#x27;rnn&#x27;]</span></span><br><span class="line"><span class="string">        config.bidirectional: 是否双向</span></span><br><span class="line"><span class="string">        config.dropout: dropout率</span></span><br><span class="line"><span class="string">        config.batch_first: 第一个维度是否是批量大小</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(RCNN, self).__init__()</span><br><span class="line">        word_embedding = word_embedding</span><br><span class="line">        self.embedding_size = config.embedding_size</span><br><span class="line">        self.embedding = nn.Embedding(config.vocab_size, config.embedding_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> config.rnn_type == <span class="string">&#x27;lstm&#x27;</span>:</span><br><span class="line">            self.rnn = nn.LSTM(self.embedding_size,</span><br><span class="line">                               config.hidden_dim,</span><br><span class="line">                               num_layers=config.n_layers,</span><br><span class="line">                               bidirectional=config.bidirectional,</span><br><span class="line">                               batch_first=config.batch_first,</span><br><span class="line">                               dropout=config.dropout)</span><br><span class="line">        <span class="keyword">elif</span> config.rnn_type == <span class="string">&#x27;gru&#x27;</span>:</span><br><span class="line">            self.rnn = nn.GRU(self.embedding_size,</span><br><span class="line">                              hidden_size=config.hidden_dim,</span><br><span class="line">                              num_layers=config.n_layers,</span><br><span class="line">                              bidirectional=config.bidirectional,</span><br><span class="line">                              batch_first=config.batch_first,</span><br><span class="line">                              dropout=config.dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.rnn = nn.RNN(self.embedding_size,</span><br><span class="line">                              hidden_size=config.hidden_dim,</span><br><span class="line">                              num_layers=config.n_layers,</span><br><span class="line">                              bidirectional=config.bidirectional,</span><br><span class="line">                              batch_first=config.batch_first,</span><br><span class="line">                              dropout=config.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1 x 1 卷积等价于全连接层，故此处使用全连接层代替</span></span><br><span class="line">        self.fc_cat = nn.Linear(config.hidden_dim * <span class="number">2</span> + self.embedding_size, self.embedding_size)</span><br><span class="line">        self.fc = nn.Linear(self.embedding_size, config.num_labels)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.dropout)</span><br><span class="line">        self.batch_first = config.batch_first</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, text, text_lengths</span>):</span></span><br><span class="line">        <span class="comment"># 按照句子长度从大到小排序</span></span><br><span class="line">        text, sorted_seq_lengths, desorted_indices = self.prepare_pack_padded_sequence(text, text_lengths)</span><br><span class="line">        <span class="comment"># text = [batch size, sent len]</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text)).<span class="built_in">float</span>()</span><br><span class="line">        <span class="comment"># embedded = [batch size, sent len, emb dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># pack sequence</span></span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_seq_lengths, batch_first=self.batch_first)</span><br><span class="line">        self.rnn.flatten_parameters()</span><br><span class="line">        <span class="keyword">if</span> config.rnn_type <span class="keyword">in</span> [<span class="string">&#x27;rnn&#x27;</span>, <span class="string">&#x27;gru&#x27;</span>]:</span><br><span class="line">            packed_output, hidden = self.rnn(packed_embedded)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># output (batch, seq_len, num_directions * hidden_dim)</span></span><br><span class="line">            <span class="comment"># hidden (batch, num_layers * num_directions, hidden_dim)</span></span><br><span class="line">            packed_output, (hidden, cell) = self.rnn(packed_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># unpack sequence</span></span><br><span class="line">        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=self.batch_first)</span><br><span class="line">        <span class="comment"># 把句子序列再调整成输入时的顺序</span></span><br><span class="line">        output = output[desorted_indices]</span><br><span class="line">        <span class="comment"># output = [batch_size, seq_len, hidden_dim * num_directionns ]</span></span><br><span class="line"></span><br><span class="line">        batch_size, max_seq_len, hidden_dim = output.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 拼接左右上下文信息</span></span><br><span class="line">        output = torch.tanh(self.fc_cat(torch.cat((output, embedded), dim=<span class="number">2</span>)))</span><br><span class="line">        <span class="comment"># output = [batch_size, seq_len, embedding_size]</span></span><br><span class="line"></span><br><span class="line">        output = torch.transpose(output, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        output = F.max_pool1d(output, <span class="built_in">int</span>(max_seq_len)).squeeze().contiguous()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.fc(output)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_pack_padded_sequence</span>(<span class="params">self, inputs_words, seq_lengths, descending=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        for rnn model ：按照句子长度从大到小排序</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        sorted_seq_lengths, indices = torch.sort(seq_lengths, descending=descending)</span><br><span class="line">        _, desorted_indices = torch.sort(indices, descending=<span class="literal">False</span>)</span><br><span class="line">        sorted_inputs_words = inputs_words[indices]</span><br><span class="line">        <span class="keyword">return</span> sorted_inputs_words, sorted_seq_lengths, desorted_indices</span><br></pre></td></tr></table></figure>
<h1 id="HAN-😊"><a href="#HAN-😊" class="headerlink" title="HAN 😊"></a>HAN 😊</h1><blockquote>
<p>论文：<a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/N16-1174.pdf">https://www.aclweb.org/anthology/N16-1174.pdf</a><br>代码：<a target="_blank" rel="noopener" href="https://github.com/richliao/textClassifier">https://github.com/richliao/textClassifier</a></p>
</blockquote>
<p>上文都是句子级别的分类，虽然用到长文本、篇章级也是可以的，但速度精度都会下降，于是有研究者提出了层次注意力分类框架，即<code>Hierarchical Attention</code>。</p>
<p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/01f2bd3e66f440e59b9445002b141090~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<p>整个网络结构包括五个部分：</p>
<ol>
<li><p>词序列编码器</p>
</li>
<li><p>基于词级的注意力层</p>
</li>
<li><p>句子编码器</p>
</li>
<li><p>基于句子级的注意力层</p>
</li>
<li><p>分类层</p>
</li>
</ol>
<p>整个网络结构由双向 <code>GRU</code> 网络和注意力机制组合而成。</p>
<h2 id="词序编码器"><a href="#词序编码器" class="headerlink" title="词序编码器"></a>词序编码器</h2><p>给定一个句子中的单词 $w_{it}$，其中 $i$ 表示第 $i$ 个句子，$t$ 表示第 $t$ 个词。通过一个词嵌入矩阵 $W_e$ 将单词转换成向量表示，具体如下所示： </p>
<script type="math/tex; mode=display">x_{it}=W_e w_{it}</script><p>将获取的词向量输入词编码器，即一个双向 <code>GRU</code>，将两个方向的 <code>GRU</code> 输出拼接在一起得到词级别的隐向量 $h$ </p>
<h2 id="词级别的注意力"><a href="#词级别的注意力" class="headerlink" title="词级别的注意力"></a>词级别的注意力</h2><p>但是对于一句话中的单词，并不是每一个单词对分类任务都是有用的，比如在做文本的情绪分类时，可能我们就会比较关注 “很好”、“伤感” 这些词。为了能使循环神经网络也能自动将“注意力”放在这些词汇上，作者设计了基于单词的注意力层的具体流程如下： </p>
<script type="math/tex; mode=display">u_{i t} =\tanh \left(W_{w} h_{i t}+b_{w}\right)</script><script type="math/tex; mode=display">\alpha_{i t} =\frac{\exp \left(u_{i t}^{\top} u_{w}\right)}{\sum_{t} \exp \left(u_{i t}^{\top} u_{w}\right)}</script><script type="math/tex; mode=display">s_{i} =\sum_{t} \alpha_{i t} h_{i t}</script><p>上面式子中，$u_{it}$ 是 $h_{it}$ 的隐层表示，$a_{it}$ 是经 <code>softmax</code> 函数处理后的归一化权重系数，$u_w$是一个随机初始化的向量，之后会作为模型的参数一起被训练，$s_i$ 就是我们得到的第 $i$ 个句子的向量表示。</p>
<h2 id="句子编码器和句子级注意力"><a href="#句子编码器和句子级注意力" class="headerlink" title="句子编码器和句子级注意力"></a>句子编码器和句子级注意力</h2><p>对于句子级别的向量，我们用相类似的方法，将其通过双向 <code>GRU</code> 和注意力层，最后将文档中所有句子的隐向量表示加权求和，得到整个文档的文档向量 $v$，将该向量通过一个全连接分类器进行分类。</p>
<h2 id="模型结构和代码-2"><a href="#模型结构和代码-2" class="headerlink" title="模型结构和代码"></a>模型结构和代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HierAttNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, rnn_type, word_hidden_size, sent_hidden_size, num_classes,word_embedding, n_layers, bidirectional, batch_first, freeze, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(HierAttNet, self).__init__()</span><br><span class="line">        self.word_embedding = word_embedding</span><br><span class="line">        self.word_hidden_size = word_hidden_size</span><br><span class="line">        self.sent_hidden_size = sent_hidden_size</span><br><span class="line">        self.word_att_net = WordAttNet(rnn_type,word_embedding, word_hidden_size,n_layers,  bidirectional,batch_first,dropout,freeze)</span><br><span class="line">        self.sent_att_net = SentAttNet(rnn_type,sent_hidden_size, word_hidden_size,n_layers,bidirectional,batch_first,dropout, num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, batch_doc, text_lengths</span>):</span></span><br><span class="line">        output_list = []</span><br><span class="line">        <span class="comment"># ############################ 词级 #########################################</span></span><br><span class="line">        <span class="keyword">for</span> idx,doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(batch_doc):</span><br><span class="line">            <span class="comment"># 把一篇文档拆成多个句子</span></span><br><span class="line">            doc = doc[:text_lengths[idx]]</span><br><span class="line">            doc_list = doc.cpu().numpy().tolist()</span><br><span class="line">            sep_index = [i <span class="keyword">for</span> i, num <span class="keyword">in</span> <span class="built_in">enumerate</span>(doc_list) <span class="keyword">if</span> num == self.word_embedding.stoi[<span class="string">&#x27;[SEP]&#x27;</span>]]</span><br><span class="line">            sentence_list = []</span><br><span class="line">            <span class="keyword">if</span> sep_index:</span><br><span class="line">                pre = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> cur <span class="keyword">in</span> sep_index:</span><br><span class="line">                    sentence_list.append(doc_list[pre:cur])</span><br><span class="line">                    pre = cur</span><br><span class="line"></span><br><span class="line">                sentence_list.append(doc_list[cur:])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sentence_list.append(doc_list)</span><br><span class="line">            max_sentence_len = <span class="built_in">len</span>(<span class="built_in">max</span>(sentence_list,key=<span class="keyword">lambda</span> x:<span class="built_in">len</span>(x)))</span><br><span class="line">            seq_lens = []</span><br><span class="line">            input_token_ids = []</span><br><span class="line">            <span class="keyword">for</span> sent <span class="keyword">in</span> sentence_list:</span><br><span class="line">                cur_sent_len = <span class="built_in">len</span>(sent)</span><br><span class="line">                seq_lens.append(cur_sent_len)</span><br><span class="line">                input_token_ids.append(sent+[self.word_embedding.stoi[<span class="string">&#x27;PAD&#x27;</span>]]*(max_sentence_len-cur_sent_len))</span><br><span class="line">            input_token_ids = torch.LongTensor(np.array(input_token_ids)).to(batch_doc.device)</span><br><span class="line">            seq_lens = torch.LongTensor(np.array(seq_lens)).to(batch_doc.device)</span><br><span class="line">            word_output, hidden = self.word_att_net(input_token_ids,seq_lens)</span><br><span class="line">            <span class="comment"># word_output = [bs,hidden_size]</span></span><br><span class="line">            output_list.append(word_output)</span><br><span class="line"></span><br><span class="line">        max_doc_sent_num = <span class="built_in">len</span>(<span class="built_in">max</span>(output_list,key=<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x)))</span><br><span class="line">        batch_sent_lens = []</span><br><span class="line">        batch_sent_inputs = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ############################ 句子级 #########################################</span></span><br><span class="line">        <span class="keyword">for</span> doc <span class="keyword">in</span> output_list:</span><br><span class="line">            cur_doc_sent_len = <span class="built_in">len</span>(doc)</span><br><span class="line">            batch_sent_lens.append(cur_doc_sent_len)</span><br><span class="line">            expand_doc = torch.cat([doc,torch.zeros(size=((max_doc_sent_num-cur_doc_sent_len),<span class="built_in">len</span>(doc[<span class="number">0</span>]))).to(doc.device)],dim=<span class="number">0</span>)</span><br><span class="line">            batch_sent_inputs.append(expand_doc.unsqueeze(dim=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        batch_sent_inputs = torch.cat(batch_sent_inputs, <span class="number">0</span>)</span><br><span class="line">        batch_sent_lens = torch.LongTensor(np.array(batch_sent_lens)).to(doc.device)</span><br><span class="line">        output = self.sent_att_net(batch_sent_inputs,batch_sent_lens)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h1 id="BERT-😊"><a href="#BERT-😊" class="headerlink" title="BERT 😊"></a>BERT 😊</h1><div align=center>
<img width="600" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e4d4a04d28ef44ec84efa17066a30ebe~tplv-k3u1fbpfcp-zoom-1.image"/>
</div>

<p><code>BERT（Bidirectional Encoder Representations from Transformers）</code> 的发布是 <code>NLP</code> 领域发展的最新的里程碑之一，这个事件 <code>NLP</code> 新时代的开始。<code>BERT</code> 模型打破了基于语言处理的任务的几个记录。</p>
<p>在 <code>BERT</code> 的论文发布后不久，这个团队还公开了模型的代码，并提供了模型的下载版本，这些模型已经在大规模数据集上进行了预训练。这是一个重大的发展，因为它使得任何一个构建构建机器学习模型来处理语言的人，都可以将这个强大的功能作为一个现成的组件来使用，从而节省了从零开始训练语言处理模型所需要的时间、精力、知识和资源。</p>
<p>更多详细内容见 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/266364526">【图解BERT】</a>、<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1389555">【图解BERT模型】</a></p>
<h2 id="Task-1-Masked-Language-Model"><a href="#Task-1-Masked-Language-Model" class="headerlink" title="Task 1: Masked Language Model"></a>Task 1: Masked Language Model</h2><p>由于 <code>BERT</code> 需要通过上下文信息，来预测中心词的信息，同时又不希望模型提前看见中心词的信息，因此提出了一种 <code>Masked Language Model</code> 的预训练方式，即随机从输入预料上 <code>mask</code> 掉一些单词，然后通过的上下文预测该单词，类似于一个完形填空任务。</p>
<p>在预训练任务中，15% 的 <code>Word Piece</code> 会被 <code>mask</code>，这 15% 的 <code>Word Piece</code> 中，80%的时候会直接替换为 <code>[Mask]</code> ，10% 的时候将其替换为其它任意单词，10% 的时候会保留原始 <code>Token</code></p>
<ul>
<li>没有100% <code>mask</code> 的原因<ul>
<li>如果句子中的某个<code>Token</code> 100% 都会被 <code>mask</code> 掉，那么在<code>fine-tuning</code> 的时候模型就会有一些没有见过的单词</li>
</ul>
</li>
<li>加入 10% 随机 <code>token</code> 的原因<ul>
<li><code>Transformer</code> 要保持对每个输入 <code>token</code> 的分布式表征，否则模型就会记住这个 <code>[mask]</code> 是 某个特定的 <code>token</code></li>
<li>另外编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个 <code>token</code> 的表示向量</li>
</ul>
</li>
<li>另外，每个 <code>batchsize</code> 只有 15% 的单词被 <code>mask</code> 的原因，是因为性能开销的问题，双向编码器比单项编码器训练要更慢</li>
</ul>
<h2 id="Task-2-Next-Sequence-Prediction"><a href="#Task-2-Next-Sequence-Prediction" class="headerlink" title="Task 2: Next Sequence Prediction"></a>Task 2: Next Sequence Prediction</h2><div align=center>
<img width="600" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4f9094e61c9049d09b118dafde9d31f1~tplv-k3u1fbpfcp-zoom-1.image"/>
</div>


<p>仅仅一个 <code>MLM</code> 任务是不足以让 <code>BERT</code> 解决阅读理解等句子关系判断任务的，因此添加了额外的一个预训练任务，即 <code>Next Sequence Prediction</code>。</p>
<p>具体任务即为一个句子关系判断任务，即判断句子B是否是句子A的下文，如果是的话输出 <code>IsNext</code>，否则输出 <code>NotNext</code>。</p>
<p>训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中 50% 保留抽取的两句话，它们符合 <code>IsNext</code> 关系，另外 50% 的第二句话是随机从预料中提取的，它们的关系是 <code>NotNext</code> 的。这个关系保存在 <code>[CLS]</code> 符号中。</p>
<h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><ul>
<li><p><code>Token Embeddings</code>：即传统的词向量层，每个输入样本的首字符需要设置为 <code>[CLS]</code>，可以用于之后的分类任务，若有两个不同的句子，需要用 <code>[SEP]</code> 分隔，且最后一个字符需要用 <code>[SEP]</code> 表示终止。</p>
</li>
<li><p><code>Segment Embeddings</code>：为 <code>[0,1]</code> 序列，用来在 <code>NSP</code> 任务中区别两个句子，便于做句子关系判断任务。</p>
</li>
<li><p><code>Position Embeddings</code> ：与 <code>Transformer</code> 中的位置向量不同，<code>BERT</code> 中的位置向量是直接训练出来的。</p>
</li>
</ul>
<div align=center>
<img width="600" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5931f6c34cc64d979ff80dc3d482d74d~tplv-k3u1fbpfcp-zoom-1.image"/>
</div>

<h2 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h2><p>对于不同的下游任务，我们仅需要对 <code>BERT</code> 不同位置的输出进行处理即可，或者直接将BERT不同位置的输出直接输入到下游模型当中。具体的如下所示：</p>
<div align=center>
<img width="600" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ec6c09b656bd4b3a8698b5ebe98a9c57~tplv-k3u1fbpfcp-zoom-1.image"/>
</div>

<ul>
<li><p>对于情感分析等单句分类任务，可以直接输入单个句子（不需要 <code>[SEP]</code> 分隔双句），将 <code>[CLS]</code> 的输出直接输入到分类器进行分类。</p>
</li>
<li><p>对于句子对任务（句子关系判断任务），需要用 <code>[SEP]</code> 分隔两个句子输入到模型中，然后同样仅须将 <code>[CLS]</code> 的输出送到分类器进行分类。</p>
</li>
<li><p>对于问答任务，将问题与答案拼接输入到 <code>BERT</code> 模型中，然后将答案位置的输出向量进行二分类并在句子方向上进行 <code>softmax</code>（只需预测开始和结束位置即可）。</p>
</li>
<li><p>对于命名实体识别任务，对每个位置的输出进行分类即可，如果将每个位置的输出作为特征输入到 <code>CRF</code> 将取得更好的效果。</p>
</li>
</ul>
<h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ul>
<li><p><code>BERT</code> 的预训练任务 <code>MLM</code> 使得能够借助上下文对序列进行编码，但同时也使得其预训练过程与中的数据与微调的数据不匹配，难以适应生成式任务。</p>
</li>
<li><p>另外，<code>BERT</code> 没有考虑预测 <code>[MASK]</code> 之间的相关性，是对语言模型联合概率的有偏估计。</p>
</li>
<li><p>由于最大输入长度的限制，适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）。</p>
</li>
<li><p>适合处理自然语义理解类任务(<code>NLU</code>)，而不适合自然语言生成类任务(<code>NLG</code>)。</p>
</li>
</ul>
<p><code>BERT</code> 分类的优化可以尝试 👼：</p>
<ul>
<li><p>尝试不同的预训练模型，比如 <code>RoBERT</code>、<code>WWM</code>、<code>ALBERT</code>。</p>
</li>
<li><p>除了 <code>[CLS]</code> 外还可以用 <code>avg</code>、<code>max</code> 池化做句表示，甚至可以把不同层组合起来。</p>
</li>
<li><p>在领域数据上增量预训练。</p>
</li>
<li><p>集成蒸馏，训多个大模型集成起来后蒸馏到一个上。</p>
</li>
<li><p>先用多任务训，再迁移到自己的任务。</p>
</li>
</ul>
<h2 id="模型结构和代码-3"><a href="#模型结构和代码-3" class="headerlink" title="模型结构和代码"></a>模型结构和代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertForSeqCLS</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, config</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(BertForSeqCLS, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(config)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(config.dropout)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">768</span> * <span class="number">3</span>, config.class_num)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, input_ids, attention_mask, labels=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># input_ids  输入的句子序列</span></span><br><span class="line">        <span class="comment"># seq_len  句子长度</span></span><br><span class="line">        <span class="comment"># attention_masks  对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]</span></span><br><span class="line">        <span class="comment"># pooled_out [batch_size, 768]</span></span><br><span class="line">        <span class="comment"># sentence [batch size, sen len,  768]</span></span><br><span class="line">        </span><br><span class="line">        outputs = self.bert(input_ids,        attention_mask=attention_mask, </span><br><span class="line">        output_hidden_states=<span class="literal">True</span>)</span><br><span class="line">        cat_out = torch.cat((outputs.pooler_output, outputs.hidden_states[-<span class="number">1</span>][:,<span class="number">0</span>], </span><br><span class="line">                             outputs.hidden_states[-<span class="number">2</span>][:, <span class="number">0</span>]), <span class="number">1</span>)</span><br><span class="line">        logits = self.fc(self.dropout(cat_out))</span><br><span class="line"></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = F.cross_entropy(logits.view(-<span class="number">1</span>, config.class_num), labels.view(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;loss&quot;</span>: loss, <span class="string">&quot;logits&quot;</span>: logits&#125;</span><br></pre></td></tr></table></figure>
<h1 id="文本分类技巧"><a href="#文本分类技巧" class="headerlink" title="文本分类技巧"></a>文本分类技巧</h1><p><img src="https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/62d4e2fd39be4a5db9471baf753f49cd~tplv-k3u1fbpfcp-watermark.image" alt="image.png"></p>
<h2 id="数据集构建"><a href="#数据集构建" class="headerlink" title="数据集构建"></a>数据集构建</h2><p>首先是标签体系的构建，拿到任务时自己先试标一两百条，看有多少是难确定（思考1s以上）的，如果占比太多，那这个任务的定义就有问题。可能是标签体系不清晰，或者是要分的类目太难了，这时候就要找项目owner去反馈而不是继续往下做。</p>
<p>其次是训练评估集的构建，可以构建两个评估集，一个是贴合真实数据分布的线上评估集，反映线上效果，另一个是用规则去重后均匀采样的随机评估集，反映模型的真实能力。训练集则尽可能和评估集分布一致，有时候我们会去相近的领域拿现成的有标注训练数据，这时就要注意调整分布，比如句子长度、标点、干净程度等，尽可能做到自己分不出这个句子是本任务的还是从别人那里借来的。</p>
<p>最后是数据清洗：</p>
<ul>
<li><p>去掉文本强 <code>pattern</code>：比如做新闻主题分类，一些爬下来的数据中带有的XX报道、XX编辑高频字段就没有用，可以对语料的片段或词进行统计，把很高频的无用元素去掉。还有一些会明显影响模型的判断，比如之前判断句子是否为无意义的闲聊时，发现加个句号就会让样本由正转负，因为训练预料中的闲聊很少带句号（跟大家的打字习惯有关），于是去掉这个<code>pattern</code> 就好了不少</p>
</li>
<li><p>纠正标注错误：简单的说就是把训练集和评估集拼起来，用该数据集训练模型两三个 <code>epoch</code> （防止过拟合），再去预测这个数据集，把模型判错的拿出来按 <code>abs(label-prob)</code> 排序，少的话就自己看，多的话就反馈给标注人员，把数据质量搞上去了提升好几个点都是可能的。</p>
</li>
</ul>
<h2 id="长文本"><a href="#长文本" class="headerlink" title="长文本"></a>长文本</h2><ul>
<li><p>任务简单的话（比如新闻分类），直接用 <code>fasttext</code> 就可以达到不错的效果。</p>
</li>
<li><p>想要用 <code>BERT</code> 的话，最简单的方法是粗暴截断，比如只取句首+句尾、句首 + <code>tfidf</code> 筛几个词出来；或者每句都预测，最后对结果综合。</p>
</li>
<li><p>另外还有一些魔改的模型可以尝试，比如<code>BERT+HAN</code>、<code>XLNet</code>、<code>Reformer</code>、<code>Longformer</code>。</p>
</li>
</ul>
<h2 id="稳健性"><a href="#稳健性" class="headerlink" title="稳健性"></a>稳健性</h2><p>在实际的应用中，鲁棒性是个很重要的问题，否则在面对 <code>badcase</code> 时会很尴尬，怎么明明那样就分对了，加一个字就错了呢？这里可以直接使用一些粗暴的数据增强，加停用词加标点、删词、同义词替换等，如果效果下降就把增强后的训练数据洗一下。当然也可以用对抗学习、对比学习这样的高阶技巧来提升，一般可以提1个点左右，但不一定能避免上面那种尴尬的情况。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.01759">https://arxiv.org/abs/1607.01759</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1408.5882">https://arxiv.org/abs/1408.5882</a></li>
<li><a target="_blank" rel="noopener" href="https://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf">https://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.5555/2886521.2886636">https://dl.acm.org/doi/10.5555/2886521.2886636</a></li>
<li><a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/N16-1174.pdf">https://www.aclweb.org/anthology/N16-1174.pdf</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/266364526">https://zhuanlan.zhihu.com/p/266364526</a></li>
<li><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1389555">https://cloud.tencent.com/developer/article/1389555</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/sandwichnlp/p/11698996.html">https://www.cnblogs.com/sandwichnlp/p/11698996.html</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/jeffery0628/text_classification">https://github.com/jeffery0628/text_classification</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/349086747">https://zhuanlan.zhihu.com/p/349086747</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35457093">https://zhuanlan.zhihu.com/p/35457093</a></li>
<li><a target="_blank" rel="noopener" href="https://www.pianshen.com/article/4319299677/">https://www.pianshen.com/article/4319299677/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/326770917/answer/698646465">https://www.zhihu.com/question/326770917/answer/698646465</a></li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">SeekingBlue</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2021/08/31/text_classification2/">http://example.com/2021/08/31/text_classification2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%B1%82%E6%A8%A1%E5%9E%8B/">深层模型</a><a class="post-meta__tags" href="/tags/BERT/">BERT</a></div><div class="post_share"><div class="social-share" data-image="https://www.vennify.ai/content/images/size/w1140/2021/06/text-classification--1--1.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/08/31/text_classification1/"><img class="next-cover" src="https://www.vennify.ai/content/images/size/w1140/2021/06/text-classification--1--1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">文本分类的深度学习方法【一】</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/08/31/text_classification1/" title="文本分类的深度学习方法【一】"><img class="cover" src="https://www.vennify.ai/content/images/size/w1140/2021/06/text-classification--1--1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-31</div><div class="title">文本分类的深度学习方法【一】</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/tx.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">SeekingBlue</div><div class="author-info__description">记录分享学习和生活的点点滴滴，陪伴彼此共同成长</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">4</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">12</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Forest-Scorpio" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/1659821119@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#DPCNN-%F0%9F%98%8A"><span class="toc-number">1.</span> <span class="toc-text">DPCNN 😊</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Region-embedding"><span class="toc-number">1.1.</span> <span class="toc-text">Region embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%9A%84%E6%9D%83%E8%A1%A1"><span class="toc-number">1.2.</span> <span class="toc-text">卷积和全连接的权衡</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%89%E9%95%BF%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.3.</span> <span class="toc-text">等长卷积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BA%E5%AE%9A-feature-map-%E7%9A%84%E6%95%B0%E9%87%8F"><span class="toc-number">1.4.</span> <span class="toc-text">固定 feature map 的数量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96"><span class="toc-number">1.5.</span> <span class="toc-text">池化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5"><span class="toc-number">1.6.</span> <span class="toc-text">残差连接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%92%8C%E4%BB%A3%E7%A0%81"><span class="toc-number">1.7.</span> <span class="toc-text">模型结构和代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#TextRCNN-%F0%9F%98%8A"><span class="toc-number">2.</span> <span class="toc-text">TextRCNN 😊</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E8%AF%8D%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.1.</span> <span class="toc-text">单词表示学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.2.</span> <span class="toc-text">文本表示学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%92%8C%E4%BB%A3%E7%A0%81-1"><span class="toc-number">2.3.</span> <span class="toc-text">模型结构和代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HAN-%F0%9F%98%8A"><span class="toc-number">3.</span> <span class="toc-text">HAN 😊</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E5%BA%8F%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">3.1.</span> <span class="toc-text">词序编码器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E7%BA%A7%E5%88%AB%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">3.2.</span> <span class="toc-text">词级别的注意力</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%A5%E5%AD%90%E7%BC%96%E7%A0%81%E5%99%A8%E5%92%8C%E5%8F%A5%E5%AD%90%E7%BA%A7%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">3.3.</span> <span class="toc-text">句子编码器和句子级注意力</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%92%8C%E4%BB%A3%E7%A0%81-2"><span class="toc-number">3.4.</span> <span class="toc-text">模型结构和代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BERT-%F0%9F%98%8A"><span class="toc-number">4.</span> <span class="toc-text">BERT 😊</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Task-1-Masked-Language-Model"><span class="toc-number">4.1.</span> <span class="toc-text">Task 1: Masked Language Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Task-2-Next-Sequence-Prediction"><span class="toc-number">4.2.</span> <span class="toc-text">Task 2: Next Sequence Prediction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5"><span class="toc-number">4.3.</span> <span class="toc-text">输入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fine-tuning"><span class="toc-number">4.4.</span> <span class="toc-text">Fine-tuning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-number">4.5.</span> <span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%92%8C%E4%BB%A3%E7%A0%81-3"><span class="toc-number">4.6.</span> <span class="toc-text">模型结构和代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%8A%80%E5%B7%A7"><span class="toc-number">5.</span> <span class="toc-text">文本分类技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA"><span class="toc-number">5.1.</span> <span class="toc-text">数据集构建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%95%BF%E6%96%87%E6%9C%AC"><span class="toc-number">5.2.</span> <span class="toc-text">长文本</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A8%B3%E5%81%A5%E6%80%A7"><span class="toc-number">5.3.</span> <span class="toc-text">稳健性</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">6.</span> <span class="toc-text">参考链接</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/08/31/text_classification2/" title="文本分类的深度学习方法【二】"><img src="https://www.vennify.ai/content/images/size/w1140/2021/06/text-classification--1--1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文本分类的深度学习方法【二】"/></a><div class="content"><a class="title" href="/2021/08/31/text_classification2/" title="文本分类的深度学习方法【二】">文本分类的深度学习方法【二】</a><time datetime="2021-08-31T03:00:00.000Z" title="Created 2021-08-31 11:00:00">2021-08-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/08/31/text_classification1/" title="文本分类的深度学习方法【一】"><img src="https://www.vennify.ai/content/images/size/w1140/2021/06/text-classification--1--1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文本分类的深度学习方法【一】"/></a><div class="content"><a class="title" href="/2021/08/31/text_classification1/" title="文本分类的深度学习方法【一】">文本分类的深度学习方法【一】</a><time datetime="2021-08-31T02:00:00.000Z" title="Created 2021-08-31 10:00:00">2021-08-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/07/22/ensemble2/" title="集成学习【二】"><img src="https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxU95ztWMrZJ4Pib02XznYG2CpPguThhzugicQsGjILibBtytW7duL35EAVtrLKtkdkFspKibcdMsmvxaw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="集成学习【二】"/></a><div class="content"><a class="title" href="/2021/07/22/ensemble2/" title="集成学习【二】">集成学习【二】</a><time datetime="2021-07-22T14:00:00.000Z" title="Created 2021-07-22 22:00:00">2021-07-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/07/20/ensemble1/" title="集成学习【一】"><img src="https://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxU95ztWMrZJ4Pib02XznYG2CpPguThhzugicQsGjILibBtytW7duL35EAVtrLKtkdkFspKibcdMsmvxaw/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="集成学习【一】"/></a><div class="content"><a class="title" href="/2021/07/20/ensemble1/" title="集成学习【一】">集成学习【一】</a><time datetime="2021-07-20T14:32:00.000Z" title="Created 2021-07-20 22:32:00">2021-07-20</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/bg2.jpg)"><div id="footer-wrap"><div class="copyright"><span>&copy;2020 - 2021</span><svg style="width:1.5em; height:1.5em" aria-hidden="true"><use xlink:href="#icon-Butterfly"></use></svg><span>SeekingBlue</span></div><div class="footer_custom_text">Grow into a complete and better me first</div><div id="workboard"></div><script async="async" src="/js/runtime.js"></script><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo"/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender"/></a><a class="github-badge" target="_blank" href="https://www.jsdelivr.com/" style="margin-inline:5px"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr"/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub"/></a></p></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script async="async">var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})
setTimeout(function(){preloader.endLoading();}, 3000);</script></div><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async src="//at.alicdn.com/t/font_2032782_8ns648avijk.js"></script><script async src="//at.alicdn.com/t/font_2264842_3izu8i5eoc2.js"></script><script defer src="/live2d-widget/autoload.js"></script><script defer src="cursor.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_clock_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src="/img/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_injector_config();
  }
  </script><script src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script data-pjax src="https://cdn.jsdelivr.net/npm/hexo-butterfly-clock/lib/clock.min.js"></script><!-- hexo injector body_end end --></body></html>